{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6617106,
          "sourceType": "datasetVersion",
          "datasetId": 3818867
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Analisis_Fraud_Transaksi_Keuangan_21110009",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HulwanulAzkaP/fraud_analisis/blob/main/Analisis_Fraud_Transaksi_Keuangan_21110009.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "zeesolver_credit_card_path = kagglehub.dataset_download('zeesolver/credit-card')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "pjAZ-WveQsQ5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:07:43.516743Z",
          "iopub.execute_input": "2025-06-12T03:07:43.517011Z",
          "iopub.status.idle": "2025-06-12T03:07:43.783567Z",
          "shell.execute_reply.started": "2025-06-12T03:07:43.51699Z",
          "shell.execute_reply": "2025-06-12T03:07:43.782851Z"
        },
        "id": "MbjYCVRaQsQ6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inisiasi Pyspark"
      ],
      "metadata": {
        "id": "MH7WAZdXQsQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengimpor pustaka yang diperlukan\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Membuat Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Fraud Detection for Credit Card Transactions\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Membaca dataset ke dalam Spark DataFrame\n",
        "df_spark = spark.read.csv('/kaggle/input/credit-card/creditcard_2023.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Menampilkan beberapa baris awal dari dataset\n",
        "print(\"Data Awal di Spark:\")\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:07:57.155903Z",
          "iopub.execute_input": "2025-06-12T03:07:57.15636Z",
          "iopub.status.idle": "2025-06-12T03:08:15.334274Z",
          "shell.execute_reply.started": "2025-06-12T03:07:57.156338Z",
          "shell.execute_reply": "2025-06-12T03:08:15.332891Z"
        },
        "id": "KuWaw4QjQsQ7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explorasi Data Analis"
      ],
      "metadata": {
        "id": "1e9bJ0cxQsQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "# Memeriksa nilai hilang di dalam dataset\n",
        "missing_data = df_spark.select([F.sum(F.col(col).isNull().cast(\"int\")).alias(col) for col in df_spark.columns])\n",
        "print(\"\\nNilai Hilang dalam Dataset:\")\n",
        "missing_data.show()\n",
        "\n",
        "# Menangani nilai null\n",
        "# Dalam dataset ini, kita anggap semua kolom harus diisi,\n",
        "# kita bisa melakukan pengisian atau penghapusan atau apa pun yang sesuai dengan analisis\n",
        "df_spark = df_spark.na.fill(0)  # Misalnya, mengisi dengan 0\n",
        "\n",
        "# Menampilkan hasil setelah pengisian\n",
        "print(\"\\nData Setelah Mengisi Nilai Hilang:\")\n",
        "df_spark.show(5)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:08:15.335779Z",
          "iopub.execute_input": "2025-06-12T03:08:15.336366Z",
          "iopub.status.idle": "2025-06-12T03:08:21.036862Z",
          "shell.execute_reply.started": "2025-06-12T03:08:15.336329Z",
          "shell.execute_reply": "2025-06-12T03:08:21.036196Z"
        },
        "id": "h-x2qG3sQsQ7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Memeriksa dan mengonversi tipe data (jika perlu)\n",
        "for col in df_spark.columns:\n",
        "    print(f\"Tipe data {col}: {df_spark.schema[col].dataType}\")\n",
        "\n",
        "# Mengonversi kolom \"Amount\" ke float jika belum\n",
        "df_spark = df_spark.withColumn(\"Amount\", df_spark[\"Amount\"].cast(\"double\"))\n",
        "\n",
        "# Pastikan kolom Class menjadi integer (0 dan 1) untuk model klasifikasi\n",
        "df_spark = df_spark.withColumn(\"Class\", df_spark[\"Class\"].cast(\"integer\"))  # 0 = legitimate, 1 = fraudulent\n",
        "\n",
        "print(\"\\nTipe Data Kolom Setelah Konversi:\")\n",
        "df_spark.printSchema()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:08:26.326597Z",
          "iopub.execute_input": "2025-06-12T03:08:26.326828Z",
          "iopub.status.idle": "2025-06-12T03:08:26.378008Z",
          "shell.execute_reply.started": "2025-06-12T03:08:26.326811Z",
          "shell.execute_reply": "2025-06-12T03:08:26.377478Z"
        },
        "id": "tPo-h8CuQsQ7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Model"
      ],
      "metadata": {
        "id": "-3Eef2tCQsQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi dataset menjadi set latih dan set uji\n",
        "train_data, test_data = df_spark.randomSplit([0.7, 0.3], seed=42)\n",
        "print(f\"Jumlah data latih: {train_data.count()}, Jumlah data uji: {test_data.count()}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:08:30.8177Z",
          "iopub.execute_input": "2025-06-12T03:08:30.817939Z",
          "iopub.status.idle": "2025-06-12T03:08:39.641089Z",
          "shell.execute_reply.started": "2025-06-12T03:08:30.817921Z",
          "shell.execute_reply": "2025-06-12T03:08:39.639397Z"
        },
        "id": "OrEqJoQFQsQ8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "# Menyusun fitur menggunakan VectorAssembler\n",
        "feature_columns = [f'V{i}' for i in range(1, 29)] + ['Amount']  # Menggunakan kolom V1-V28 dan Amount\n",
        "\n",
        "assembler = VectorAssembler(inputCols=feature_columns, outputCol='features')\n",
        "train_data = assembler.transform(train_data)\n",
        "test_data = assembler.transform(test_data)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:09:35.524384Z",
          "iopub.status.idle": "2025-06-12T03:09:35.524891Z",
          "shell.execute_reply.started": "2025-06-12T03:09:35.524499Z",
          "shell.execute_reply": "2025-06-12T03:09:35.524732Z"
        },
        "id": "W8i_9FL3QsQ8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model"
      ],
      "metadata": {
        "id": "xVs19bOPQsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Membuat model Random Forest\n",
        "rf = RandomForestClassifier(featuresCol='features', labelCol='Class', numTrees=100)\n",
        "\n",
        "# Melatih model\n",
        "model_rf = rf.fit(train_data)\n",
        "\n",
        "# Melakukan prediksi\n",
        "predictions = model_rf.transform(test_data)\n",
        "\n",
        "# Menilai model\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol='Class', predictionCol='prediction', metricName='accuracy')\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(f'Akurasi model: {accuracy:.2f}')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:08:50.701608Z",
          "iopub.execute_input": "2025-06-12T03:08:50.702437Z",
          "iopub.status.idle": "2025-06-12T03:09:35.384395Z",
          "shell.execute_reply.started": "2025-06-12T03:08:50.70241Z",
          "shell.execute_reply": "2025-06-12T03:09:35.38342Z"
        },
        "id": "2wUu0RCXQsQ8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualisasi Hasil Training"
      ],
      "metadata": {
        "id": "1JPu9myXQsQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "# Mengonversi prediksi ke Pandas DataFrame untuk visualisasi\n",
        "predictions_pd = predictions.select(\"Class\", \"prediction\").toPandas()\n",
        "\n",
        "# Menghitung confusion matrix\n",
        "confusion_matrix = pd.crosstab(predictions_pd['Class'], predictions_pd['prediction'], rownames=['Actual'], colnames=['Predicted'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Legitimate', 'Fraudulent'], yticklabels=['Legitimate', 'Fraudulent'])\n",
        "plt.xlabel('Prediksi')\n",
        "plt.ylabel('Sebenarnya')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:09:56.437631Z",
          "iopub.execute_input": "2025-06-12T03:09:56.438018Z",
          "iopub.status.idle": "2025-06-12T03:10:03.661768Z",
          "shell.execute_reply.started": "2025-06-12T03:09:56.437996Z",
          "shell.execute_reply": "2025-06-12T03:10:03.661173Z"
        },
        "id": "WWwAHWCXQsQ8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x='prediction', data=predictions_pd)\n",
        "plt.title('Distribusi Prediksi Model')\n",
        "plt.xlabel('Prediksi (0: Legitimate, 1: Fraudulent)')\n",
        "plt.ylabel('Jumlah')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-12T03:10:03.662675Z",
          "iopub.execute_input": "2025-06-12T03:10:03.662994Z",
          "iopub.status.idle": "2025-06-12T03:10:03.762257Z",
          "shell.execute_reply.started": "2025-06-12T03:10:03.66298Z",
          "shell.execute_reply": "2025-06-12T03:10:03.761581Z"
        },
        "id": "spbG_IIUQsQ8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kesimpulan Pelatihan Model Deteksi Penipuan\n",
        "Akurasi Model: Model yang dilatih mencapai akurasi sebesar 95%. Ini menunjukkan bahwa model berhasil mengklasifikasikan transaksi menjadi 'legitimate' dan 'fraudulent' dengan sangat baik, artinya sebagian besar prediksi model adalah benar.\n",
        "\n",
        "# Confusion Matrix:\n",
        "\n",
        "True Positives (TP): 77910 transaksi fraudulent teridentifikasi dengan benar.\n",
        "True Negatives (TN): 84853 transaksi legitimate teridentifikasi dengan benar.\n",
        "False Positives (FP): 401 transaksi legitimate yang salah terklasifikasi sebagai fraudulent.\n",
        "False Negatives (FN): 7423 transaksi fraudulent yang salah terklasifikasi sebagai legitimate.\n",
        "Dari confusion matrix, terlihat bahwa meskipun model memiliki kesalahan dalam mengklasifikasi beberapa transaksi, proporsi True Positives dan True Negatives yang tinggi menunjukkan validitas model.\n",
        "\n",
        "# Distribusi Prediksi:\n",
        "\n",
        "Perbandingan prediksi menunjukkan bahwa jumlah transaksi legitimate jauh lebih tinggi dibandingkan fraudulent, yang merupakan karakteristik umum dalam dataset transaksi kartu kredit.\n",
        "Model dapat dengan efektif mendeteksi fraudulent transactions meskipun proporsi mereka lebih kecil.\n",
        "Potensi Peningkatan:\n",
        "\n",
        "Meskipun model menunjukkan akurasi tinggi, ada ruang untuk perbaikan lebih lanjut. Khususnya, mengurangi jumlah False Negatives akan sangat penting, karena ini berarti transaksi fraudulent tidak terdeteksi dan berpotensi mengakibatkan kerugian bagi lembaga keuangan.\n",
        "Metode lain seperti SMOTE (Synthetic Minority Over-sampling Technique) untuk menangani ketidakseimbangan kelas atau teknik ensemble lainnya bisa dipertimbangkan untuk meningkatkan performa.\n",
        "Rekomendasi Tindak Lanjut:\n",
        "\n",
        "Melakukan validasi silang untuk lebih memastikan daya tahan model pada data yang berbeda.\n",
        "Menggunakan metrik lain seperti precision, recall, dan F1-score untuk memberikan pemahaman lebih dalam tentang performa model dalam konteks deteksi fraud.\n",
        "Implementasi model dalam produksi dan monitor performanya secara berkala, menyesuaikan apabila ada perubahan dalam pola transaksi."
      ],
      "metadata": {
        "id": "H_hpye0oQsQ8"
      }
    }
  ]
}